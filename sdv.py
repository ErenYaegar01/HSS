import json new pom.xml from sdv.datasets.demo import download demo 
from sdv.datasets.demo import get available demosfrom sdv.multi_table 
import HMASynthesizer from sdv.single_table import GaussianCopulaSynthesizer 
from sdv.evaluation.multi table import DiagnosticReport 
from sdv.evaluation.multi table import run diagnostic 
from sdv.evaluation.multi table import evaluate quality 
from sdv.evaluation.multi table import get column plot 
from sdv.evaluation.multi_table import get column pair plot 
from sdv.utils import drop unknown references  
from sdv.metadata import MultiTableMetadata, SingleTableMetadata 
from datetime import datetime 
from urllib.parse import quote import pandas as pd 
import pymysql import sqlite3 
from google.cloud import bigquery 
import yaml 20 from yaml.loader import SafeLoader 
import streamlit authenticator as stauth 
import extra streamlit components as stx 
import hydralit components as ht 
from code editor import code editor 
import time  import os 
import streamlit as st 
import pandas as pd 
import base64 30 import subprocess 
from PIL import Image import re 
from sdmetrics.reports.multi_table 
import QualityReport from sdmetrics.reports.multi_table 
import DiagnosticReport 
import logging from io import StringIo

#Database connection- To store and retrieve system settings conn 
#sqlite3.connect('synthetic.db') 
# curs = conn.cursor() #Retrieve the basic settings and store it in session state to preserve the values during navigation and refreshing st.session state.folder_path= curs.execute("select Value from Settings where Type 'Baze folder'").fetchall() [0] [0] st.session state.GCP_Schema curs.execute("select Value from Settings where Type GCF_Schema").fetchall() [0] [0] st.session state.GCP_Project curs.execute("select Value from Settings where Type GCP_Project").fetchall () [0] [0] st.session state. Single Table_Synthesizer curs.execute("select Value from Settings where Type Single Table_Synthesizer'").fetchall() [0] [0] st.session state.Mult_Table_Synthesizer curs.execute("select Value from Settings where Type="Mult Table Synthesizer'").fetchall () [0] [0] if 'data_multi' not in st.session state: st.session_state.data_multi-10 if 'f time not in st.session state: st.session_state.f_time = True Streamlit page configuration I st.set_page_config(layout='wide', initial_sidebar_state=collapsed",) with open('sec.yaml') as file: config yaml.load(file, Loader-SafeLoader) Logo and Portal title at top authenticator stauth. Authenticate ( logo_sec st.columns([3,1]) ) config['credentials'], config['cookie'] ['name'], config['cookie'] ['key'], config['cookie'] ['expiry_days'], config['pre-authorized'] with logo sec[0]: st.header("***Synthetic Data Generator***") with logo sec [1]: st.image("images/logo.png", width=200)- fauthenticator.login()


acumsMNG.txt x Credentials.txt new 4X pom.xml x Sheet 2-S 75 76 77 ### # # if(st.button("user")):  log_sec=st.tabs(["EMail", "GCB"]) authenticator.login() 78 if st.session_state.f_time==True: 79 st.write("#") 80 st.write("#") 81 82 83 84 85 86 87 88 89 90 91 st.write("#")

t_cols-st.columns(3) with t cols [1]: with st.container (border=True): image Image.open("images/GCP.png") new image image.resize((200, 50)) st.image (new_image) if (st.button("Login with your GCP credentials")): with st.spinner('Waiting for login..."): return code subprocess.call("gcloud auth application-default login", shell-True) st.session state.lg_user=subprocess.getoutput('geloud config list account format "value (core.account) --verbosity error') st.session state ["authentication_status"]-True st.session state login_type="GCP st.session_state.f_time-False st.session_state ["name"]="Admin" st.rorun() if login is successfull if st.session_state ["authentication_status": ###Menu selection for Admin user if (st.session_state ["name"]"Admin"): menu data1 ['icon': "bi bi-robot", "label":"Model Training", "ttip': "Train new model with your own data"), ['icon': "bi bi-clipboard-data", 'label': "Generate Data"), ('icon': "far fa-chart-bar", 'label":"Matrics"), fa-user", submenu": [['id': 'add_user', {"icon": "fas fa-tachometer-alt", 'label': "Usage Report", 'ttip':"SDV ("icon": "far "label":"Security", [' Settings!"), "icon": Ican add a tooltip message "fa fa-plus", 'label': "Add Uaer"). id':'pwd_reset', 'icon': "fa fa-unlock", " label":"Password Reset"), ('id': 'update user', 'icon': "fa fa-pencil", 'label': "Update User"}}}, (' icon': "icon': "fa fa-paperclip". "bi bi-gear-fill", "label":"Settings", "submenu": [{'id': 'mata setting"," "label":"Metadata Update"), ('id': 'oth_setting', 'icon': "fa fa-paperclip", "label: "Custom Constraints")," ('id': 'sys setting', "icon': "fa fa-database", 'label': "System Settings"))),

["icon": "fa fa-question-circle", 'label': "Help"), J 117 Menu selection for non-admin users (no user mangement, statistics and system settings options) else: ATE menu data [ 1201 101 ('icon': "bi hi-robot", 'label': "Model Training", "ttip': "Train new model with your own data"), ("icon": "bi bi-clipboard-data", "label":"Generate Data"), 122 ('icon': "far fa-chart-bar", 'label': "Metrics"), ('icon 123 274 ': "bi bi-gear-fill", "label":"Settings", "submenu" [['id': 'meta_setting","icon": "fa fa-paperclip", "label":"Metadata Update). ('id': 'oth_setting', 'icon": "fa fa-paperclip", 'label: "Custom Constraints"}]}, ("icon": "fa fa-question-circle", "label":"Help"), 125 126 1 127 ###nisplaying menu bar in streamlit window lover theme ('txc_inactive': '#FFFFFF') 129 190 over theme ('txc_inactive': 'white', 'menu_background:#CC0000*) 131 menu idhe.nav_bar( 332 menu definition-menu_data, override theme over theme, 133 134 135 home name="Home", login_name='Logout', hide streamlit markers-True, #will show the st hamburger as well as the navbar now! sticky_nav=False, fat the top or not sticky_mode='pinned', #jumpy or not-jumpy, but sticky or pinned ###Home Page Settings if menu id="Home": fat.write(f'Welcome (st.session_state["name"])**) #align col-st.columns (4) with align_col (3) align_colest.columns(3) st.title("Welcome To") with align col [2]: st.write(f'Logged in as (st.session_state.lg_user) sunglasses:") 144 46 #with align_col=140 150 151 153 Normal text file Type here to search align col-st.columns([2,4,2])
st.title("Synthetic Data Generator Portal") 153 with align col [1]: 154 255 158 157 153 176 179 102 105 st.header("About this portal", divider=True) Set 2-Sacan to new 3 bxf st.write("This portal helps you in providing high-quality synthetic data to meet your testing, training, and analytical needs. Whether you're developing machine learning models, testing software, or conducting research, our synthetic data ensures privacy. compliance, and accuracy.") st.header("What is Synthetic Data?", divider-True) st.write("Synthetic data is artificially generated information that mimics real-world data while ensuring the privacy and security of the original data. It offers a risk-free solution for organizations to develop, test, and validate their systems without compromising sensitive information.") st.header("Features", divider=True) align_cols-st.columns (3, gap-large') with align_cols[0]: st.write("## Data Variety") st.write("Generate data across various domains, including finance, healthcare, retail, and more.") with align_cols [1]: st.write("### Privacy and Security") st.write("Our synthetic data safeguards personal and sensitive information, ensuring compliance with GDPR, HIPAA, and other regulations") with align_cols[2]: st.write("Quality and Accuracy") st.write("We use advanced algorithms to create realistic and reliable data tailored to your specific needs.") align_cols-st.columns (3, gap-large") with align_cols[0]; st.write(" Customization") st.write("Allows you to define the parameters and characteristics of the synthetic data, providing a bespoke solution for your unique rege with align_cols [1]: st.write("## Scalability") st.write("Generate large volumes of data effortlessly, enabling extensive testing and robust model training") with align cols [2]: st.write(" Train Custom Models") st.write("Connect to the database/Upload your dataset and train a model tailored to your needs.") align_cols=st.columns (3,gap-'large') 180 189 180 191 with align cols[0]:

st.title("Synthetic Data Generator Portal") 153 with align col [1]: 154 255 158 157 153 176 179 102 105 st.header("About this portal", divider=True) Set 2-Sacan to new 3 bxf st.write("This portal helps you in providing high-quality synthetic data to meet your testing, training, and analytical needs. Whether you're developing machine learning models, testing software, or conducting research, our synthetic data ensures privacy. compliance, and accuracy.") st.header("What is Synthetic Data?", divider-True) st.write("Synthetic data is artificially generated information that mimics real-world data while ensuring the privacy and security of the original data. It offers a risk-free solution for organizations to develop, test, and validate their systems without compromising sensitive information.") st.header("Features", divider=True) align_cols-st.columns (3, gap-large') with align_cols[0]: st.write("## Data Variety") st.write("Generate data across various domains, including finance, healthcare, retail, and more.") with align_cols [1]: st.write("### Privacy and Security") st.write("Our synthetic data safeguards personal and sensitive information, ensuring compliance with GDPR, HIPAA, and other regulations") with align_cols[2]: st.write("Quality and Accuracy") st.write("We use advanced algorithms to create realistic and reliable data tailored to your specific needs.") align_cols-st.columns (3, gap-large") with align_cols[0]; st.write(" Customization") st.write("Allows you to define the parameters and characteristics of the synthetic data, providing a bespoke solution for your unique rege with align_cols [1]: st.write("## Scalability") st.write("Generate large volumes of data effortlessly, enabling extensive testing and robust model training") with align cols [2]: st.write(" Train Custom Models") st.write("Connect to the database/Upload your dataset and train a model tailored to your needs.") align_cols=st.columns (3,gap-'large') 180 189 180 191 with align cols[0]:


st.write(" Generate High-Quality Synthetic Data") st.write("Produce synthetic data that mimics the characteristics of your original dataset.") with align cols [1]: st.write(" Evaluate and Compare Metrics") MNG 192 193 194 195 196 197 199 200 201 202 21 211 st.write("Analyze the quality and performance of the generated synthetic data.") st.header("How it works?", divider True) st.write("1. **Train a Model: Upload your dataset or GCP data, and configure the training parameters.") data.") st.write("2. **Generate Data**: Use the trained model to generate synthetic st.write("3. **Metrics**: Review metrics and analyze the synthetic data quality.") st.header("App in Action", divider=True) st.write("Sample Screenshots") #st.image("", caption="Model Training Page") st.write("Demo Video") #st.video ("") Documentation and Support st.header("Need Help?", divider=True) st.write("Check out our [documentation) (https://link-to-docs) or contact us at support@example.com.") st.write("") st.write("") ###Usage report page settings elif menu id="Usage Report": st.header("Audit Logs") audit_hist=pd.read_sql_query("SELECT FROM AUDIT LOG", conn) st.table (audit_hist) fusage management page settings elif menu id="add user": st.header("User Management") #st.write(" Note: Below functionalities are under construction") if (st.button("Add User")): email_of_registered_user, username_of_registered_user, name_of_registered user if email_of_registered_user: authenticator.register_user(pre_authorization-Fa 229 Normal text file st.toast('User registered successfully') else: st.toast('error registering member')

pomat Sura new 3.txt 3 with open('sec.yaml', 'w') as file: yaml.dump(config, file, default_flow_style=False) st.button("Remove User") #st.button ("Update User") #if (st.button("Unlock User/Password Reset")): # if authenticator.reset_password(st.session_state("username"]): st.success (Password modified successfully") elif menu id="pwd reset": user_name=st.text_input ("Enter user name") if authenticator.reset_password(user_name): st.success("Password modified successfully') with open('sec.yaml', 'w') as file: else: yaml.dump(config, file, default_flow_style=False) st.toast('error resetting password") #Logout settings elif menu id="Logout": st.write("Logging Out...") if(st.session_state.login_type!-"GCP"): authenticator.logout (location'unrendered") ##clearing all the session values keys list(st.session state.keys()) for key in keys: st.session_state.pop(key) st.rerun() ###Model Training page settings elif menu id="Model Training": st.warning("**Important: This app processes and stores sensitive user information. By using this app. you agree to share your data and understand that it will be used for model training.") ###UI element to display numbered steps step val stx.stepper_bar(steps-["Mode Selection", "Define Data Source", "Training Config", "Train")) 267 268 261 dbname=** metadata list=""

Steel 2-Satan bf now 3.txt 3 ##Model selection step 1 if step valmo: db namewat.selectbox("Select the table option", ("single Table", "Multi Table", "Excel Source")) st.session state['db_name']db_name st.caption("***single Table Choose this option if you just want to train a single table (table with no referential integrity)***) st.caption("***Multi Table- Choose this option if you train a more than table (table with referential integrity)****) st.caption("* Excel Source- Choose this option if your data source is excel sheet****) # #Data source selection step 2 elif step val-1: db_namowat.session_state['db_name'] st.session state.uploaded_file st.file_uploader ("Choose a file") if (db nam"Excel Source") #Display DB connection inputs only if the slected option is not an excel source if db_name 1 "Excel Source": inputs sectionist.columns (2) st.session state.table schema-st.selectbox("Schema", st.session_state,GCP-Schema.split(",")) with inputs section1[0]; st.session state.project_id=st.selectbox("GCP Project", st.session_state.GCP_Project.split(",")) with inputs section) [1]: session state.tab radbutton=st.radio("select the table", ["INDIVIDUAL", "RACE", "ETHNICITY", "CITIZENSHIP", "LANGUAGE", "T INDIVIDUAL SUB PREFERENCE"), horizontal-True) if (db name"Multi Table"): if (db_name="Single Table"): checks st.columns(6) with checks [0]: state.ind_checkBox=st.checkbox("INDIVIDUAL", True) st.session with checks [1]: st.session state.rac_checkBox=st.checkbox ("RACE") with checks [2]: st.session state.eth_checkBox-st.checkbox ("ETHNICITY") with checks[3]: state.cit_checkBox-st.checkbox("CITIZENSHIP") st.session with checks [4]: st.session state.lan_checkBox-st.checkbox ("LANGUAGE") with checks [5]: st.session state.sub_pref_checkBoxest.checkbox("T_INDIVIDUAL_SUB_PREFERENCE") st.write("#") 
 st.write("#")
 st.write("#")
#Inputs for training configuration step 3 elif step_val-2: db_name=st.session_state['db_name'] if db name "Excel Source": inputs_section2 st.columns (2) st.session_state.data_limit="" with inputs section2 [0]: st.session state.metadata_choice-st.selectbox("Create/Use Existing Metadata", ("Use Existing", "Generate froms scratch"), with help "Metadata file defines the table structure") inputs_section2[1]: 16 31 311 320 321 322 323 324 else: 32 326 32 328 32 340 41 st.session state.metric_choice#st.selectbox("Do You want the metric report", ("yes", "no") help-Choose "yes", if you want to evaluate the model quality') inputs section2 st.columns (3) with inputs_section2 [01: st.session state.data_limit=st.text_input("Training Data Limit", "10", help="No of rows utilized for training. Less value-> More speed & less accuracy and vice versa") with inputs_section2[1]: st.session state.metadata_choice=st.selectbox("Create/Use Existing Metadata", ("Generate froms scratch", "Use Existing"), with help="Metadata file defines the table structure") inputs_section2 [2]: st.session state.metric_choice-st.selectbox("Do You want the metric report", ("yes", "no"), help="Choose "yes", if you want to evaluate the model quality') st.write("") st.session_state.metadata_name-** ##Create metadata file #if st.button("Click here to generate Metadata File", help="Click this button, if you just want to generate Metadata file and dont want to train the ML model): if "a"-"b": ###Use SingleTableMetadata class to generate metadata file, if the table type is Single Table if (db name "Single Table"): biqquery.Client (location="US", project-st.session_state.project_id) client - real data {} sql f"SELECT FROM. (st.session df-client.query_and_wait(sql).to_dataframe () state.table_schema).(st.session_state.tab_radButton) limit (st.session_state.data_limit)" real_data[st.session state.tab radButton) df metadata singleTableMetadata() metadata.detect from dataframe (df)

drew 43 pom ml 3 Sheet2-Saranbd 2 new 3.txt metadata.save to_json(f"(st.session_state.folder_path)/Metadata/(st.session_state.tab_radButton).json") st.toast("MetaData file generated. Navigate to Metadata setting screen to review.") ### elif (db_name "Multi Table"): Use MultiTableMetadata class to generate metadata file, if the table type is Multi Table client bigquery.Client (location="Us", project=st.session_state.project_id) real_data {} #table_schema="edp-dev-storage.edp_ent_core_src" #tables- ("CITIZENSHIP", "RACE", "INDIVIDUAL", "ETHNICITY"] tables-[] 350 351 352 353 354 355 356 357 358 359 if st.session_state.ind_checkBox True: tables.append("INDIVIDUAL") if st.session_state.rac_checkBox True: tables.append("RACE") True: if st.session_state.cit_checkBox if st.session_state.lan_checkBox True: tables.append("T_INDIVIDUAL LANGUAGE") 369 370 71 306 tables.append("CITIZENSHIP") if st.session_state.eth_checkBox True: tables.append("ETHNICITY") if st.session_state.sub_pref_checkBox True: tables.append("T_INDIVIDUAL_SUB_PREFERENCE") model name" ".join(str(table) for table in tables) *2r9q5wKKnfkB', 'QEtkIAnctpkr', '46ufY6rJSBTL', 'LJDVFF6JgVB7) 23330H3', 'V2VVC9DPHLft', 'leNPv571jYiA', 'LaHkDyMibDuh", "wjpnNIqfXEk8', 'r tndpBbwmcc", Running select query against each table using the valid list of keys retrieved from previous step for table in tables: #df pd.read_sql (f"SELECT 5q1f"SELECT FROM FROM (table schema) (table) limit 100;", con db_connection) {st.session_state.table_schema). (table) WHERE individualIdentifier_idValue IN ({st.session_state.params)) limit (st.session_state.data_limit)" df-client.query_and_wait (sql).to_dataframe() df real_data[table] metadata MultiTableMetadata () metadata.detect_from_dataframes(real_data) current_datetime datetime.now() current_date_time current datetime.strftime("im_sd_SY_SH_SM_43") model name model name +""+ current date time

metadata.save_to json(f"[st.session_state.folder_path)/Metadata/(model_name).json") st.toast(statectadata_name-f"[st.session_state.folder_path)/Metadata/(model_name}.json" st.toast ("MetaData file generated. Navigate to Metadata setting screen to review.") if at session state.metadata_choice=="Use Existing": align_cols=st.columns([2,6,2]) metadata list-os.listdir(f"(st.session_state.folder_path)/Metadata") st.session state.metadata_name-st.selectbox("Select the Metadata file", metadata_list) 290 302 894 395 with align_cols [1]: 397 # if db name != "Excel Source": 399 401 VZVVC9DPHLft, 1CNPv571jYiA, LaHkDyMibDth, wipaNlqfXEks, Endpbbwmcc, 2r9q5wKKnfkB, QEtkIAnctpkr, 46UfY6rJSBTL, JDV1f6JqVB7", 402 403 1 st.session state.params','.join(f"" (e)*" for e in params.split(",")) print (params) when you use this.") help="Training Data limit will be ignored when 404 elif step val-3: 405 406 107 #params- st.text_input("Enter the list of primary keys (seperated by comma(,)", "9Y2VCSS3p0H3, db_name-st.session_state['db_name'] oth comment="Metadata File:" + st.session_state.metadata_name + "\n" oth_comment=oth comment "Training Data Limit" st.session_state.data_limitin align_cols=st.columns (5) with align_cols [2]: if st.button("Initiate Training", type='primary'): print("h") s_time-datetime.now() st.session state['button'] = False if (db_name"Single Table"): progress text = "Fetching the data from database. Please wait." my bar st.progress (0, text progress text) client- bigquery.client (location="US",project-st.session_state.project_id) real data() sql = f"SELECT FROM (st.session_state.table_schema).(st.session_state.tab_radButton) limit (st.session state.data_limit) df-client.query_and_wait(sql).to_dataframe() real data[st.session state.tab_radButton] - df metadata SingleTableMetadata() if st.session_state.metadata choice=="Generate froms scratch": metadata.detect_from_datafrane (df) metadata.save_to_json(f"(st.session_state.folder_path)/Metadata/(st.session_state.tab_radButton).json") else: metadata-metadata.load from json(f"(st.session state. folder path)/Metadata/(st.session state.metadata name)")

now 3.bxt E3 my_bar.progress (10, text-"Retrieved details. Plotting Table Structure...") time, sleep (2) with st.container (height=500, border-True): st.write("Table Structure") st.graphviz_chart (metadata, visuali ize (show_table_details'full')) my_bar.progress (30, text "Retrieved details. Initiating Model Training") time.sleep(2) synthesizer = GaussianCopulasynthesizer (metadata, locales=['en_US"]) my bar.progress (50, text-"Model Training in progress") synthesizer.fit (df) my bar.progress (80, text="Training completed. saving the Model...") time.sleep(2) current datetime datetime.now() current date time current_datetime.strftime("\n_d_Y_SH_SM_55") D synthesizer.save(filepath-f"(st.session_state.folder_path}/Models/(db_name)_{st.session_state.tab_radButton)_(current_date ate_time)_model.pk1") my_bar.progress (80, text="Training Completed! Evaluating Model") #st.info("Training is completed") st.toast (f"Models (db_name)_model.pkl is stored", icon'1") elif (db_name"Multi Table"): percent complete-0 progress text "Fetching the data from database. Please wait." my bar st.progress(0, text progress_text) df = pd.read_sql("SELECT table_name FROM information schema.tables WHERE table schema" db #print (df) ***", con-db_connection) client bigquery.Client(location="US", projectest.session_state.project_id) real data() #table schema "edp-dev-storage.edp_ent core sre" # tables["CITIZENSHIP", "RACE", "INDIVIDUAL", "ETHNICITY"] tables=[] if st.session_state.ind_checkBox True: tables.append("INDIVIDUAL") if st.session_state.rac_checkBox tables.append("RACE") True: st.session_state.cit cheakBox True: tables.append("CITIZENSHIP") if st.session_state.lan_checkBox True: tables.append("T INDIVIDUAL LANGUAGE") 


if st.session_state.eth checkBox True: tables append("ETHNICITY") 469 470 if st.session_state.sub_prof_checkBox True: tables.append("T_INDIVIDUAL SUB_FREFERENCE") model_name"".join(str(table) for table in tables) 471 472 474 print("tables;" + model_name) #sample records 477 479 402 154 502 50 sql - "SELECT individualIdentifier_idValue FROM edp-dev-storage.edp_ent_core_src. INDIVIDUAL limit 50 query job client.query_and_wait (sql) #df-query_job.to_dataframe() params tuple (df ['individualIdentifier_idValue'].values) #params(9Y2vCS53p083, V2VVC9DPHLft, ICNPv571jYiA", "LaHkDyMibDth', 'wjpnNlqfXEK8", 'r tndpBbwmcc", *2r9q5wKKnfkB', 'QEtKIAnctpkr', '46ufY6rJSBTL', 'tJDVff6JqVB7")" #getting valid key values using join query # 541-f SELECT distinct 1.individualIdentifier_idValue FROM edp-dev-storage.edp_ent_core_src: INDIVIDUAL AS I inner join edp-dev-storage.edpent core sre.RACE AS Lon I.individualIdentifier_idValue-L.individualIdentifier idValue inner join edp dev-storage.edp_ent_core_sro. ETHNICITY AS E on L.individualIdentifier_idValue E.IndividualIdentifier_idValue inner join edp-dev-storage.edp_ent_core_src.CITIZENSHIP AS Con B.individualIdentifier_idValue-I.individualIdentifier_idValue limit 10 sql-f"SELECT distinct L.individualIdentifier idValue FROM edp-ga-storage.edp_ent_anbc_het. INDIVIDUAL AS I inner join edp-ga-storage.edp_ent_anbe_het. RACE AS Lon I.individualIdentifier_idValue-L.individual Identifier_idValue inner join "edp-qa-storage.edp_ent_anbc_het.ETHNICITY AS E on L. individualIdentifier_idValue-E.individualIdentifier_idValue inner join edp-qa-storage.edp_ent anbe het.CITIZENSHIP AS Con E. individualidentifier_idValue C.individualIdentifier_idValue limit (st.session state.data_limit # sql-f"SELECT distinct I. individualIdentifier_idValue FROM edp-ga-storage.edp_ent_anbe_het. INDIVIDUAL AS I inner join 'edp-ga-storage.edp_ent_anbc_het.T_INDIVIDUAL LANGUAGE AS Lon S on L.individualpreferencecatalogid-s.individualpreferencecatalogid limit (at.session state.data limit)" LindividualIdentifier_idValue-L.individual preferencecatalogid inner join edp-qa-storage.edp_ent_anbc_het.T INDIVIDUAL SUB PREFERENCE AS print (sql) val_id list_tmp-client.query_and_wait(sql).to_dataframe () val_id_list tuple (val_id_list_tmp['individualIdentifier_idValue').values) creating source data using the valid key values retrieved for table in tables: df-pd.DataFrame() I for val id in val id list:

pomnil Shout 2-Saran tet 32 now 3.txt print("val id: val_id) if table in ("T_INDIVIDUAL LANGUAGE", "T INDIVIDUAL SUB PREFERENCE"): else sql ("SELECT FROM st.session_state.table_schema).(table) WHERE individualpreferencecatalogid (val_id)' limit 1" sql "SELECT FROM (st.session_state.table schema), (table) WHERE individualIdentifier_idValue(val id)' limit 1" print (sql) dfl-client.query_and_wait(sql).to dataframe() df=df._append (df1) if table !="INDIVIDUAL": for col in df.columns: if "datetimeBegin" in col or "datetimeAsof" in col: df [col] = df [col].apply(lambda x: pd.to_datetime(x).strftima (SY-Am-SdTSH:SM:65')) print (col+" is done") real_data[table]-df real data [table].to_csv(f"(table).csv") print (real _data) metadata MultiTableMetadata() print(st.session_state.metadata_choice) if st.session_state.metadata_choice="Generate froms scratch": metadata.detect_from_dataframes (real_data) current_datetime datetime.now() current date time current_datetime.strftime("Sm Sd SY SH SM 85") metadata.save_to_json(f"(st.session state.folder_path)/Metadata/(model_name)_(current_date_time].json") if st.session_state.cit_checkBox==True: metadata.update_column(table_name='CITIZENSHIP', column_name='sgk_citizenship_id", sdtype='id') print("inside citizen") metadata.update_column (table_name='CITIZENSHIP', column_name'individualIdentifier_idValue', sdtype='id') metadata.remove_primary_key(table_name='CITIZENSHIP') metadata.remove_relationship("INDIVIDUAL", "CITIZENSHIP") metadata.add_relationship ( parent_table_name='INDIVIDUAL', child_table_name='CITIZENSHIP, parent_primary_key='individualIdentifier_idValue', child foreign key-'individualIdentifier idValue')

Sheet 2 Saranbt &rmy2 now 3 bf if st.session_state.ind_checkBox True: metadata.update_column(table_name='INDIVIDUAL', column_name-'individualIdentifier_idValue', sdtype 'id') metadata.remove_primary_key(table_name="INDIVIDUAL) metadata.set primary_key(table_name='INDIVIDUAL, column_name=individualIdentifier_idValue*) if st.session state.eth_checkBox True: metadata.update_column (table_name='ETHNICITY', column_name' individualIdentifier idValue', sdtype='id') metadata.remove_primary_key(table_name-'ETHNICITY') metadata.add_relationship parent_table_name' INDIVIDUAL, child_table_name="ETHNICITY, parent_primary_key' individualIdentifier_idValue', child_foreign_key='individualIdentifier_idValue if st.session_state.rac_checkBox==True: metadata.update_column (table_name-'RACE', column_name=individualIdentifier_idValue', sdtype-id*) metadata.remove_primary_key(table_name='RACE') metadata.add_relationship ( parent_table_name='INDIVIDUAL, child_table_name='RACE, parent primary_key='individualIdentifier_idValue", child_foreign_key=individualIdentifier_idValue' ) if st.session state.lan_checkBox True: metadata.update_column (table_name='T_INDIVIDUAL_LANGUAGE", column_name=individualpreferencecatalogid", sdtype="id") metadata.remove_primary_key(table_name='T_INDIVIDUAL LANGUAGE") metadata.add_relationship( parent_table_name='INDIVIDUAL", child_table_name='T INDIVIDUAL LANGUAGE, parent primary_key='individualIdentifier_idValue", child_foreign_key='individualpreferencecatalogid')


demant Shoct 2 if st.session_state.lan_checkBox True: - Sarannow 3 bxt 509 590 591 $92 593 595 596 597 else: 598 599 600 601 602 603 604 005 507 metadata.update_column (table_name='T_INDIVIDUAL_SUB_PREFERENCE", column_names individualpreferencecatalogid, adtype'id') metadata.remove_primary_key(table_name='T_INDIVIDUAL_SUB_PREFERENCE") metadata.add_relationship( parent table name='INDIVIDUAL', child_table_name='T INDIVIDUAL SUB PREFERENCE', parent primary_key-'individualIdentifier_idValue", child_foreign_key='individualpreferencecatalogid' ) metadata-metadata.load_from_json(f"(st.session_state.folder_path)/Metadata/(st.session state.metadata_name}") fed met st.text_area ("Metadata", metadata) Imetadata.save_to_json("test.json") print("meta data done") print("step 3") metadata.validate_data (data=real_data) my_bar.progress (10, text-"Retrieved details. Plotting Table Structure...") time.sleep(2) with st.expander ("Table Structure"); with st.container (height=500, border-True): st.write("Table Structure") st.graphviz_chart (metadata.visualize(show table_details='full')) my_bar.progress (30, t text-"Retrieved details. Initiating Model Training") time.sleep(2) #real_data-drop_unknown_references (real_data, metadata) my_constraint_1= { *constraint_class ': 'FixedCombinations", 'table_name': 'CITIZENSHIP', for multi table synthesizers ' constraint parameters": 'column_names': ['Country_code', 'Country_fullName"] my_constraint 2 ( 'constraint_class': 'FixedCombinations', 'table name: 'RACE', for multi table synthesizers


'constraint parameters': [ 'column_names': ['code', 'raceDescription"] my_constraint_3=1 constraint_class': 'FixedCombinations", * for multi table synthesizers ' table_name': 'ETHNICITY', ' *column_names': ['code', 'ethnicityDescription') constraint parameters': #my_constraint 4- ( 'constraint class': 'FixedCombinations", 'table name': 'T INDIVIDUAL LANGUAGE, for multi table synthesizers *constraint_parameters: ( 'column names': ['code', 'description"] synthesizer HMASynthesizer (metadata, locales=['en_US')) synthesizer.add_constraints ( constraints-[ my_constraint_1.my_constraint_2,my_constraint_3 synthesizer HMASynthesizer (metadata, locales=["en_US"]) my_bar.progress (50, text-"Model Training in progress") synthesizer.fit(real_data) my_bar.progress (80, text="Training completed. Saving the Model...") time.sleep(2) current_date_time current_datetime.strftime("im sd_SY_SH_SM_S5") synthesizer.save(filepath-f"(st.session_state.folder_path)/Models/(db_name)_(model_name)_(current_date_t time) model.pkl") current datetime datetime.now() table_row counts() for table_name, df in real data.items(): row count df.shape[0] oth comment =oth comment "training data count:"+ json.dumps (table row counts) + "\n"

23 223 new 3 bxt Ist.info("Training is completed") st.toast("Models (db_name)_(model_name) (current_date_time)_model.pkl is stored", icon="1") "Model Name:" + f"(db_name) (model_name)_(current_date_time)_model.pk1" + "\n" oth commentsoth comment print(oth comment) Evaluating the model if st.session_state.metric_choice'yes': my bar.progress (80, text-"Training Completed! Evaluating Model") synthetic_data synthesizer.sample(scale=1) print (synthetic data) synthetic data["INDIVIDUAL").Lo caw(f"INDIVIDUAL_r.csv") synthetic data["RACE").to_csv(f"RACE r.csv") synthetic_data["ETHNICITY"].to_cav ("ETHNICITY_r.csv") #diag_report DiagnosticReport() #qual report Quality Report () my bar.progress (90, text="Genrating diagnostic report") time.sleep (2) diag_report-run diagnostic(real data real data, synthetic data-synthetic_data, metadata-metadata) #diag_report.generate (real data, synthetic data, metadata) diag report.save(filepath-f"(st.session_state.folder_path)/Reports/[db_name)_(model_name)_(current_date_time)_diagnostic_report.pkl") my_bar.progress (95, text="Generating quality report") #qual_report.generate (real data, synthetic data, metadata) qual report-evaluate_quality(real data-real_data, synthetic_data synthetic_data, metadata-metadata) qual_report.save(filepath=f"[st.session_state.folder_path}\Reports\{db_name)_(model_name)_(current_date_time)_quality_report.pkl") st.write("Diagnostic Score") diag_score diag_report.get_properties () diag report model DiagnosticReport.get_details (property_name='Data Validity') diag_score['Score'] diag score['Score'].apply(lambda x: x*100) Normal text file Type here to search st.write(diag_score) st.write(diag_report_model) fat.write("Quality Score") #qual score qual_report.get_properties() #qual score-pd.concat(qual score_d.values(), ignore_index=True) #qual score['Score'] qual_score['Score').apply(lambda x: x*100) #st.write(qual score) st.info("Check the metrics section for detailed comparison")

new 4 ponuml Sboel 2-Samnitt ryw 2 new 3 bet st.write (f****Real column vs. Synthetic column****) tabl, tab2 st.tabs(["Ethnicity", "Race"]) with tabl: figget column_plot( real data real_data, synthetic_data-synthetic_data, metadata-metadata, table_name-ETHNICITY", column_name='ethnicityDescription" st.write(fig) with tab2: fig get_column_plot( real data real data, synthetic_data synthetic data, metadata-metadata, table_name='RACE', column_name='raceDescription st.write(fig) my_bar.progress (100, text-"Completed") else: percent_complete=0 progress_text "Fetching the data from excel file. Please wait." my_bar st.progress (0, text-progress_text) real data = pd.read_excel(st.session_state.uploaded_file, sheet name=None) print (real_data) model name db name metadata MultiTableMetadata () if st.session_state.metadata_choice "Generate froms scratch": print("new metadata") metadata.detect_from_dataframes (real_data) metadata.remove_primary_key(table_name='CITIZENSHIP') metadata.update_column (table_name-CITIZENSHIP', column_name='sgk_citizenship_id', Normal text file Type here to search sdtype='id')


pom.xml Sheet 2-Saran.txt new 3. txt 776 777 778 779 sdtype 'id') metadata.update_column (table_name=*CITIZENSHIP', column_name' individual Identifier idValue', metadata.update_column (table_name='INDIVIDUAL, column_name='individualIdentifier idValue, sdtype='id') metadata.remove_primary_key(table_name='INDIVIDUAL') metadata.set_primary_key(table_name='INDIVIDUAL',column_name='individualIdentifier_idValue') sdtype='id') metadata.update_column(table_name='ETHNICITY', column_name='individualIdentifier idValue', metadata.remove_primary_key(table_name='ETHNICITY') metadata.add_relationship( parent_table_name='INDIVIDUAL', child_table_name='ETHNICITY', parent_primary_key='individualIdentifier_idValue', child_foreign_key='individualIdentifier idValue' ) metadata.update_column (table_name='RACE', column_name='individualIdentifier_idValue', sdtype='id') metadata.remove_primary_key(table_name='RACE') metadata.remove_primary_key(table_name='T_INDIVIDUAL_LANGUAGE') metadata.add_relationship( parent_table_name='INDIVIDUAL', child_table_name='RACE', parent_primary_key='individualIdentifier_idValue' child_foreign_key='individualIdentifier_idValue' ) metadata.add_relationship (parent_table_name='INDIVIDUAL', child table_name='CITIZENSHIP', parent_primary_key='individual Identifier_idValue', child foreign_key='individualIdentifier idValue' )

Sheet 2 Saran tot now3bt datetime.now() current_datetime current datetime.strftime("in Sd SY SH SM ) current date time metadata.save_to_json[f"(st.session state.folder_path)/Meladata/(db nane)_(current_date_time).json") print("old metadata") metadata-matadata.load from json(f"[st.session_state.folder_path)/Metadata/[st.session state.metadata_name)") ed metest.text_area ("Metadata", notadata) print("meta data done") print("step 3") 762 783 724 795 metadata.add_relationship (parent_table_name INDIVIDUAL, child_table_name='T INDIVIDUAL LANGUAGE", parent primary_key' individual Identifier_idValue", child foreign key-'individualidentifier 786 767 788 789 790 else: 791 792 print (db name) 193 754 795 797 799 001 103 604 105 806 108 1 my_bar.progress(10, text="Retrieved details. Plotting Table Structure...") time.sleep (2) with st.container (height-500,border-True): st.write("Table Structure") st.graphviz_chart (metadata.visualize(show_table_details'full')) my_bar.progress (30, text="Retrieved details. Initiating Model Training") time.sleep(2) my_constraint_1 = [ ' constraint class': 'FixedCombinations', "table_name': 'CITIZENSHIP", for multi table synthesizers 'constraint_parameters":{ 'column_names': ['Country_code', 'Country_fullName'] my_constraint_2 = | 'constraint class': 'FixedCombinations', ' table_name': 'RACE', for multi table synthesizers ' constraint_paramsters': ( 'column_names': ['code', 'raceDescription'] my constraint_3={  'constraint class': 'FixedCombinations',


pom.xml 3 Sheet 2-Saran bd 3 ruw 2 now 3. bxt 'table name': 'ETHNICITY', 'constraint parameters': ( for multi table synthesizers 'column_names': ['code', 'ethnicityDescription']' my_constraint_4 = { 'constraint_class ': 'FixedCombinations" 'table_name': 'T_INDIVIDUAL LANGUAGE, for multi table synthesizers 'constraint_parameters': 1 'column_names': ['code', 'description'] synthesizer = HMASynthesizer(metadata, locales=['en_US']) synthesizer.add_constraints (constraints=[ 1) my_constraint_1,my_constraint_2,my_constraint_3 my_bar.progress (50, text="Model Training in progress") synthesizer.fit (real_data) my_bar.progress (80, text="Training completed. Saving the Model...") w1 acums/MNG.bxf Credentials txt 3 new t 821 822 823 824 025 826 827 828 829 830 032 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 049 1150 651 852 synthetic data synthesizer.sample(scale-1) 853 654 855 856 857 858 85% < Normal text file Type here to search time.sleep(2) current_datetime datetime.now() current_date time current_datetime.strftime("%m &d SY SH &M SS") oth_comment=oth_comment + "Model Name" + f" (db_name)_(model_name}_model.pkl" + "\n" if thiet-save(filepath=f"{st.session_state.folder_path}/Models/(db_name}_{model_name)_(current_date_time)_model.pk1" st.session_state.metric_choice=='yes': my_bar.progress (80, text="Training Completed! Evaluating Model") Ist.info("Training is completed") st.toast(f"Models (db_name)_(model_name)_model.pkl is stored", icon='1') #Evaluating the model #diag_report DiagnosticReport () my_bar.progress (90, text="Genrating diagnostic report") time.sleep(2) metadata MultiTableMetadata() metadata.detect_from_dataframes(real data) def diagnostic(real data, synthetic data, metadata):

Credentials bd 3 new 4 pom.xml Sheet 2-Saran bxt 3 new 2x new 3.txt 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 861 882 883 885 686 887 880 609 8,90 192 193 195 197 diagnostics = {} completeness Rate total_values real data.shape non_missing_values real_data.count().sum() completeness_rate (non_missing_values)/(total values) 100 diagnostics['Completeness Rate'] = completeness rate #Accuracy Rate correct_values (real_data synthetic_data).sum().sum() total_values real_data.shape accuracy_rate = (correct_values / total_values) 100 diagnostics['Accuracy Rate'] accuracy_rate return diagnostics diag_report=run_diagnostic(real_data-real_data, synthetic_data=synthetic_data, metadata-metadata) st.title("Data Validity Report!!") for metric, score in diag_report.items(): st.subheader("Data Validation!") st.progress (score / 100) st.write(f"(metric): (score:.2f)%") #st.subheader ("Running Diagnostic..") #progress st.progress (0) #status_text st.empty() stages = [ 'Evaluating Data Validity...', "Evaluatinsg Data Structure... 'Evaluating Relationship Validity..... for i, stage in enumerate (stages): status_text.text(stage) time.sleep (2) progress.progress((i + 1) (80 // len(stages))) ") #st.subheader ("Running Diagnostic.... ## qual_report Quality Report() qual_report.generate (real_data, synthetic_data, metadata) st.write("1/3 Evluating Data Validity:") #validity_progress st.progress (0) st.write("2/3 Evaluating Data Structure:") structure progress = st.progress(0)

Credentires tot now 48 pormonit Sheet 2 Saran bf now 2 new 3 txt 3 901 902 903 904 905 906 907 get_score() data_validity_score diag_report.get 908 909 910 911 912 time.sleep (1) 913 914 progress.progress(100) #st.write("3/3 Evaluating Realtion Validity:") #relationship_progress st.progress (0) diag_report-run_diagnostic(real_data-real_data, synthetic_data=synthetic_data, metadata-metadata) Data Validity for i in range (101): time.sleep(0.01) validity progress.progress(i) st.write(f"Data Validity Score: (data_validity_score:.3f)s") overall_score diag_report.get_score() progress.progress (90) #status text.text(f'Overall Score: (overall_score)') 915 #status text.text('Diagnostic report Completed!!!") 916 st.subheader('Diagnostic Report') #st.write(diag_report) 917 918 (95, myreposavilepath=f"(st.session_state.folder_path)/Reports/fdb-name)_(model_name)_(current_date_time)_diagnostic_report.pel) 919 920 #qual_report.generate (real data, synthetic data, metadata) qual_report-evaluate_qualityfreal_data-real_data, synthetic_data synthetic_data, 921 metadata-metadata) my_bar.progress (100, qual report.save(filepath-f"(st.session_state.folder_path)/Reports/(db_name)_(model_name)_(current_date_time)_quality_report.pki*) 922 923 st.write("Diagnostic Score") 924 925 926 927 929 930 931 933 936 figget column_plot( diag_score=diag_report.get_properties() diag_score['Score'] "diag_score['Score'].apply(lambda x: x*100) st.write(diag_score) st.write("Quality Score") qual_score qual_report.get_properties() #qual_score=pd.concat(qual_score_d.values(), ignore_index=True) qual_score['Score'] qual_score['Score'].apply(lambda x: x*100) st.write(qual_score) st.info("Check the metrics section for detailed comparison") st.write(f****Real column vs Synthetic column***") tabl, tab2, tab3 st.tabs (["Ethnicity", "Race", "Citizenship"]) with tabl: real data real data,synthetic _data = synthetic _data, table_name = 'RACE', column_name = 'raceDescription') st.write(fig)


st.write(fig) ) poma Sheet 2-Saran bat 32 now 3.txt 953 954 955 956 957 958 959 960 with tab3: figget column_plot( real data-real_data, synthetic data-synthetic_data, notadata-metadata, table_name='CITIZENSHIP", column_name="Country_code 961 962 with tab4: 963 65 66 967 969 at.write(fig) else: #below test line 902 904 905 987 900 950 fig get_column_plot( real data real_data, synthetic_data-synthetic_data, metadata metadata, table_name='T_INDIVIDUAL_LANGUAGE', column_name-description" my_bar.progress (100, text="Completed") #synthetic data synthesizer.sample (scale=1) e_time-datetime.now() Time_differencese_time-s_time diff minutes divnod (Time_difference.total_seconds(), 60) time taken str(diff minutes[0]) + 'minutes str(diff minutes[1]) "seconds str(time), stree)TO AUDI LOG VALUES (NULL, ?, ?, ?, "Model Training, ?,?,?)", (str(st.session_state("name")),[ str(e_time), str (db_name), time taken, str(oth_comment))) conn.commit() log stream StringIO () logging.basicConfig(stream-log_stream, level-logging. INFO, format="% (message)s") logger logging.getLogger() st.title("Console Log Viewer") elif (menu_id'Generate Data'); model list-os.listdir(f"(st.session state.folder path)/Models")

celsst.coltans (2) with ocl[0]; model namesst.selectbox("select the Model", model_list) with cel[l]: if if mod detail: della curs.execute(f"select Other_Info from AUDIT LOG where action="Model Training and Other Info like '[model_name}\"").fetchall) 994 996 997 990 1002 else: 1008 1009 match-re.search(r'\(([^)]*)\)', str (mod_detail)) tab_rec_data-match group (1) Ist.write("") st.code(match group (1)) Jat.write("") st. code ("Table Data Not Available") tab_rec_dat data ccl-at.columns (2) with cel[0]: dat_multiwst.text_input("Required synthetic data", st.session_state.data_multi,help- synthetic data count this of original data count") with cell): if tab_rec_data: rea json.loada ("(" + tab_rec_data + ")") first value next (iter (zes.values())) recent-int(int (first_value) *int (dat_multi)/100) Ist.write("") Ist.write (f"Total primary table record in the output: {rec_cnt)") st.text input("How much prinary table record you will get", recent, disabled-True) st.session state.data_multi dat multi ocl-st.columns (2) with cel(0): table schemawat.selectbox("Schema", st.session_state.OCP_Schema.split(",")) with crl[l]: project idest.selectbox("GCP Project", st.session_state.GCP_Project.split(",")) st.session state.data_multi dat multi 1012 201 101 1015 1016 1017 4028 1019 1020 1021 1022 2023 1025 1020 for i in range Idat (dat_multi): time.slamp(0.05) 1020 1020 1030 Jormal text fie Type here to search progress barst progress (0) progress bar.progress((1+1)/100) st.success("Data Generated Sucessfully!!") dat multiwint (dat multi)/100


print("start time:") 1032 1033 print (datetime.now()) oth comment "Model:" model_name "\n" 1034 ccll-st.columns (2) 2035 with ccll[0]: 1036 1037 1030 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 tab_names list (synthetic_data.keys()) st.session state.synth-synthetic_data tabs st.tabs (tab_names) with tabs[i]: st.write(f"(table_name) Data:") oth_comment=oth_comment table_name+":"+ str(len(df)) 1058 1059 1060 print (datetime.now()) if st.button("Generate Synthetic Data"): st.session_state['button'] False s_time datetime.now() if(model_name.startswith("Single Table")): synthesizer-GaussianCopulasynthesizer.load(filepath=f"[st.session_state.folder_path)/Models/(model_name)") synthetic data synthesizer.sample(num_rows=10) felif (model_name.startswith("Multi Table")): else: #with he.HyLoader('',he.Loaders.standard_loaders, height-5, index=[2,2,2]): with st.status ("Generating Synthetic Data..., expanded-True): with he. HyLoader(", he.Loaders.pulse_bars,): synthesizer HMASynthesizer.load(filepath-f"(st.session_state.folder_path)/Models/(model_name}") print (dir (synthesizer)) synthetic data synthesizer.sample (scale-float (dat_multi)) for i, (table_name, df) in enumerate (synthetic_data.items(), start-0): st.dataframe (df) st.session_state.tot_tables-i print("end time:") 1061 e_time datetime.now() 1062 1063 1064 1065 1066 3067 1060 1069 Time_difference-e_time-s_time diff minutes divmod (Time difference.total_seconds(), 60) time_taken str(diff_minutes[0])+ 'minutes str(diff minutes[1]) *seconds #make an audit log entry Curs execute("INSERT INTO AUDIT LOG VALUES (NULL, ?,?,?, Synthetic Data Generation', ?, ?,7)", (str(st.session state["name"]), str(s_time), str(e_time), str(dat_multi*100),time_taken, str(oth_comment))) conn.commit() st.write(st.session state.tot tables)


accumMNG with cell[1]: . td Credentials bd row 43 pom vol 3 Sheet 2-Sarantit 32 now 3 bxt if st.button("Push the data to DB"): #st.write(st.session_state.synth) client bigquery.Client (location="US", project project_id) for i, (table name, df) in enumerate (st.session_state.synth.items(), start-0): table_name_upd-table_schema+","+table_name" SYNTH DATA POC my_bar st.progress (0, text="Uploading Started") tot_count-st.session state.tot_tables+1 print (table_name_upd) print (f" (i) (tot_count) (int{{(i+1)/tot_count) *100))") client.delete_table (table_name_upd) #time.sleep (2) #time.sleep (2) print("uploaded") my_bar.progress(int(((i+1)/tot_count) 100)-20, text="Deleteing exsisting table -> table_name_upd) my_bar.progress(int(((i+1)/tot_count) *100), text="Uploading the data for table->" + table_name_upd) job= client.load_table_from_dataframe (df, table_name_upd) my_bar.progress (100, text="Completed") st.toast("Data are uploaded into GCP") elif (menu_id="Metrics"): model_list=os.listdir ("Models") 1071 1072 1073 1074 1075 1076 1077 1078 1079 1000 1091 1002 1083 1084 1085 1086 1007 1086 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 tables_list 1101 with d tab: 1102 1103 1104 1105 1106 1107 1100 1305 < Normal text file Type here to search model_name-st.selectbox("Select the Model", model list) if st.button ("View Report"): st.session_state['button'] False db_name-model_name.replace("_model.pkl","") d_tab, qual_tab=st.tabs(["Diagnostic Report", "Quality Report"]) synthesizer-HMASynthesizer.load(filepath=f"(st.session_state.folder_path}/Models/(model_name}") parsed_data json.loads (str (synthesizer.get_metadata())) d_report DiagnosticReport.load(f"(st.session_state.folder_path)/Reports/(db_name)_diagnostic_report.pkl") diag scored_report.get_properties() list (parsed_data['tables'].keys()) diag score['Score') -diag_score['Score').apply(lambda x: x*100) st.write(diag score) dv, rv-st.tabs (["Data Validity", "Relationship Validity"]) with dv: for tab name in tables list: st.write( f"***Table: (tab name)***")


Credenbals bd 23 new 43 pom.xml Sheet 2-Saran bd 32 new 3 bxt figd report.get visualization ( ) property_name'Data Validity', table_name=tab_name st.write(fig) st.markdown (*** BoundaryAdherence: This metrics measures whether a synthetic column respects the minimum and maximum values of the real column. It returns the percentage of synthetic rows that adhere to the real boundaries. Data Compatibility: Score: Numerical This metric is meant for numerical data. Datetime: This metric converts datetime values into numerical values. If you have missing values in the real data, then the metric will consider them valid in the synthetic data. Otherwise, they will be marked as out-of-bounds. (best) 1.0: All values in the synthetic data respect the min/max boundaries of the real data. ( worst) 0.0: No value in the synthetic data is in between the min and max value of the real data. The graph below shows an example of some fictional real and synthetic data (black and green, respectively) with BoundaryAdherence=0.912. CategoryAdherence: This metric measures whether a synthetic column adheres to the same category values as the real data. (The synthetic data should not be inventing new category values that are not originally present in the real data.) Data Compatibility: 1138 113 1140 1142 1142 1143 Score: 1144 1145 1146 1147 1148 < Normal text file Type here to search Categorical: This metric is meant for discrete, categorical data. Boolean: This metric is meant for boolean data. If you have missing values in the real data, then the metric will consider them valid in the synthetic data. Otherwise, they will be marked as an invalid category. All types of missing values (NaN, None, etc. will be counted as the same category of 'missing".) (best) 1.0: All category values in the synthetic data were present in the real data (worst) 0.0: None of the category values in the synthetic data were present in the real data Any score in between tells us the proportion of data points that are adhering to the correct values. For example, 0.6 means that 60% of synthetic data points have a value present in in the real data. Meanwhile, the remaining 40% contain new values that were never present in the real data.""",unsafe_allow_html=True)


tet Credential to now pom Sheet 2-Saran od 2 new 3 tbt with rv: for tab name in tables list: st.write( f***Table: (tab_name]***") figd_report.get_visualization( property_name='Relationship Validity', table_name=tab_name 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1104 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 2201 1182 1103 1164 1195 1357 < Normal text file Type here to search st.write(fig) st.markdown ( ReferentialIntegrity: This metric measures the integrity of a connection between a foreign key and primary key. Every value in the foreign key column must be found in the primary key. Data Compatibility: Foreign Key: This metric is meant for foreign keys. Primary Key: This metric validates that the foreign key values are found in the primary key. counts missing values as valid foreign keys. This metric Score: (best) 1.0: All the foreign key values are found in the primary key ( Worst) 0.0: None of the foreign key values are found in the primary key. This indicates that the dataset has orphan children, which is invalid in most database systems. CardinalityBoundaryAdherence: If there are two connected tables, the cardinality refers to the number of connections between a parent row and the child. This metric measures whether the cardinality of the synthetic data follows the min/max values as determined by the real data. Data Compatibility: Foreign Key: This metric is meant for foreign keys. Primary Key: This metric validates that the foreign key values are found in the primary key. This metric ignores missing values in the foreign key. Score: (best) 1.0: The cardinality of the synthetic data is always in the min/max bounds as determined by the real data. ( worst) 0.0: The cardinality of the synthetic data is never whether the min/max bounds. KeyUniqueness: This metric measures whether the keys in a particular dataset are unique. We expect that certain types of keys, such as primary keys, are always unique in order to be valid.


Data Compatibility: ID: This metric is meant for ID data. w 3.txt 3 Other: This metric can work with any other type of semantic data that is used in place of an ID, such as natural key like mail. Score: with qual tab: (beat) 1.0: All of the key values in the synthetic data are unique. (worst) 0.0: None of the key values in the synthetic data are unique report DiagnosticReport.load(f"Reports/(db_name) quality_report.pki") qual score q_report.get properties() qual score['Score'] qual score['Score").apply(lambda x: x*100) st.write(qual_score) cs,cpt,card, itsst.tabs(['Column shapes', 'Column Fair Trends', 'Cardinality', 'Intertable Trends']) with.cs: for tab name in tables list: st.write(**Table: (tab_name)****) figq_report.get_visualization( property_name-Column Shapes', table name=tab_name st.write(fig) st.markdown( G 1210 1220 1221 1222 1223 1224 1225 1226 1227 TVComplement: This metric computes the similarity of a real coluin vs. synthetic column in terms of the column shapes, the marginal distribution or ID histogram of the column. Data Compatibility: Categorical: This metric is meant for discrete, categorical data. Boolean: This metric works well on hoolean data. This metric ignores any missing values. Score: (best) 1.0: The real data is exactly the same as the synthetic data. (worst) 0.0: The real and synthetic data are as different as they can be.

Credents 34 pomem Sheet 2- Saran bitny KSComplement: new 3 txt This The marginal distribution or 10 histogram of the column. metric computes the similarity of a real column vs. a synthetic column in terms of the column shapes. Data Compatibility: Numerical: This metric is meant for continuous, numerical data. Datetime: This metric converts datetime values into numerical values. This metric ignores missing values. Score: ( best) 1.0: The real data is exactly the same as the synthetic data. (worst) 0.0: The real and synthetic data are as different as they can be. """, unsafe_allow_html=True for tab_name in tables_list: st.write( f"***Table: (tab_name)") figq_report.get_visualization( property_name='Column Pair Trends', table_name tab name ) st.write(fig) st.markdown ( 1242 124 with cpt: 1244 1245 124 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 ) 1264 with card: < Correlationsimilarity: This metric measures the correlation between a pair of numerical columns and computes the similarity between the real and synthetic data- --aka it compares the trends of 2D distributions. This metric supports both the Pearson and Spearman's rank coefficients to measure correlation. Data Compatibility: Numerical: This metric is meant for continuous, numerical data. Datetime: This metric converts datetime values into numerical values. This metric ignores missing values. Score: (best) 1.0: The pairwise correlations of the real and synthetic data are exactly the same. (worst) 0.0: The pairwise correlations are as different as they can possibly be.
""")

new 3 txt with card: for tab name in tables list: st.writel fig qreport.get_visualization( f****Table: (tab_name)****) property_name='Cardinality'. table_name-tab name st.write(fig) st.markdown( CardinalityShapeSimilarity: If you have multi table, connected tables, this metric measures whether the cardinality of the parent table is the same between the real and synthetic datasets. The cardinality is defined as the number of child rows for each parent. Data Compatibility: ID: This metic is meant to be used on ID columns (primary and foreign keys). Primary key IDs must be unique while foreign key IDs can repeat. Score: (best) 1.0: The cardinality values are the same in the real and synthetic data. (worst) 0.0: The cardinality values are as different as can be. 12661 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 2277 1278 1279 1280 1282 1282 1203 1284 1285 1286 1287 1208 1289 1290 1291 1292 1293 1294 1295 1296 1297 1299 1299 2300 302 1302 1303 1304 < Normal text file Type here to search with it: for tab name in tables list: st.write( Table: (tab_name)***) fig q report.get visualization( property_name='Intertable Trends', table_name=tab_name st.write(fig) st.markdown ( Intertable Trends: This property denormalizes the parent and child table into a single, flat table. Then, it applies the same metrics as the Column Fair Trends property.


) Contingencysimilarity: new 3 bxf This metric computes the similarity of a pair of categorical columns between the real and synthetic datasets. It compares 2D distributions. Data Compatibility: Categorical: This metric is meant for discrete, categorical data. Boolean: This metric works well on boolean data. To use this metric, both of the columns must be compatible. If there are missing values in the columns, the metric will treat then as an additional, single category. Score: (best) 1.0: The contingency table is exactly the same between the real vs. synthetic.data. (worst) 0.0: The contingency table is as different as can be. CorrelationSimilarity: This metric measures the correlation between a pair of numerical columns and computes the similarity between the real and synthetic data aka it compares the trends of 2D distributions. This metric supports both the Pearson and Spearman's rank coefficients to measure correlation. Data Compatibility: Numerical: This metric is meant for continuous, numerical data. Datetime: This metric converts datetime values into numerical values. This metric ignores missing values. Score: (best) 1.0: The pairwise correlations of the real and synthetic data are exactly the same. (worst) 0.0: The pairwise correlations are as different as they can possibly be. elif (menu_id="meta_setting") st.title("Update Your Table MetaData File") model list-os.listdir(f"(st.session_state.folder_path)/Metadata") model_name-st.selectbox("Select the Metadata file to edit", model list) fopen(f" (st.session state.folder_path)/Metadata/(model_name}") mot cont-f.read() f.close() code editor config variables
height =[19, 22]

Credenfas d Shost 2-Saran bir *showWithIcon": True, "commands": ["returnselection"], "style": ("bottom": "0.44rem", "right": "0.4rem") 2new 3 bxt new json code editor (met_cont, lang="json", theme-"dark", response_node="blur") fresponse dict code_editor (met_cont, height = height, lang-language, theme theme, shortcuts-shortcuts, focus focus, buttons=editor_btns, options={"wrap": wrap)) MNG bit t 1344 1345 language="json" theme "default" 1346- shortcuts="vscode" 1347 focus-False 7340 wrap-True 1340 editor_btns-11 1350 "name": "Run", 1351 "feather": "Play", 1352 "primary": True, 1353 "hasText": True, 1354 1355 1356 1357 1356 1359 1360 1361 1362 1363 4 show response dict st.write(response_dict) # 1364 1365 1366 1367 st.write(new json) 1369 1369 1370 1371 1372 2373 1374 1375 1376 1377 1379 1379 3380 2381 1302 < if len(response_dict['id']) != 0 and (response_dict["type"] "selection" or response dict['type']"submit"): #new_json-st.text area ("Metadata", value json.load(f)) #new sonst_ace (met_cont) if st.button("Update"): with open(f"(st.session state.folder_path]/Metadata/(model_name}", 'w') as outfile: outfile.write(new_json["text"]) elif (menu_id="sys_setting"): st.header("System Settings") Base_folder-st.text_input("Base folder path", st.session_state.folder_path) GCP_Schema-st.text input("GCP Schema list (seperated by comma)",st.session_state.GCP_Schema) GCP_Project=st.text_input("GCP Project list (seperated by comma)", st.session state.GCP_Project) Single Table Synthesizerest.text_input("Single Table Synthesizer Type", st.session_state.single_Table_Synthesizer) Mult_Table Synthesizer-at.text input("Multi Table Synthesizer Type", st.session state.Mult_Table_Synthesizer) if (st.button("Save")): curs.execute("update Settings set Value-? where Type 'Base folder'", (Base_folder,)) curs.execute("update Settings set Value-? where Type-OCP_Schema", "(GCP_Schema,)) curs.execute("update Settings set Value? where Type GCP Project", (OCP Project,)) curs.execute("update Settings set Value=? where Type 'Single Table Synthesizer'", (Single Table Synthesizer,))

Credentials bt now 4 pom xml Sheet2-Saran bt & new 23 new 3 bet 1383 MNG tit curs.execute("update Settings set Value=? where Type-'Mult_Table_Synthesizer'", (Mult_Table_Synthesizer,}} 1384 conn.commit() 1385 1386 1387 1386 1389 1390 1391 1392 1393 1994 1395 1396 1397 1398 1399 1400 st.write("") st.session state.folder_path=Base_folder st.toast ("Settings Updated", icon-') file="help.pdf" elif (menu_id="Help"): with open(file, "rb") as f: base64 pdf base64.b64encode (f.read()).decode ("utf-8") Embedding PDF in HTML pdf_display = F'<iframe src="data:application/pdf;base64, (base64_pdf)" width="1000" height="1000" type="application/pdf"></iframe> Displaying File st.markdown (pdf_display, unsafe_allow_html=True) elif st.session_state["authentication_status"] is False: st.error('Username/password is incorrect') elif st.session_state ["authentication_status"] is None: st.warning("Please enter your username and password')